diff --git a/ainara/framework/chat_manager.py b/ainara/framework/chat_manager.py
index bb47c4c..dce95ee 100644
--- a/ainara/framework/chat_manager.py
+++ b/ainara/framework/chat_manager.py
@@ -38,6 +38,7 @@ from ainara.framework.loading_animation import LoadingAnimation
 from ainara.framework.orakle_middleware import OrakleMiddleware
 from ainara.framework.template_manager import TemplateManager
 from ainara.framework.tts.base import TTSBackend
+from ainara.framework.user_profile_manager import UserProfileManager
 from ainara.framework.utils import load_spacy_model
 
 # import pprint
@@ -105,6 +106,15 @@ class ChatManager:
             self.chat_memory = None
             logger.info("Chat memory disabled")
 
+        # Initialize User Profile Manager
+        if self.chat_memory:
+            self.user_profile_manager = UserProfileManager(
+                llm=self.llm, chat_memory=self.chat_memory
+            )
+            logger.info("User Profile Manager initialized")
+        else:
+            self.user_profile_manager = None
+
         # Render the system message template
         current_date = datetime.now().date()
         self.system_message = self.template_manager.render(
@@ -768,6 +778,8 @@ class ChatManager:
     def chat_completion(
         self, question: str, stream: Optional[Literal["cli", "json"]] = "cli"
     ) -> Union[str, Generator[str, None, None], dict]:
+        user_message_id = None
+        assistant_message_id = None
         """Main chat completion function
 
         Args:
@@ -810,7 +822,7 @@ class ChatManager:
         processed_answer = ""
         try:
             if self.chat_memory:
-                self.chat_memory.add_entry(question, "user")
+                user_message_id = self.chat_memory.add_entry(question, "user")
 
             # Check if the last message is from a user, and if so, log a warning
             if (
@@ -843,8 +855,33 @@ class ChatManager:
                 self.chat_history[1]["tokens"] = new_summary_tokens
                 logger.info(f"Applied new summary: {new_summary_copy[:50]}...")
             self.trim_context()
+
+            # --- User Profile Injection ---
+            turn_chat_history = self.chat_history
+            if self.user_profile_manager:
+                relevant_beliefs = (
+                    self.user_profile_manager.get_relevant_beliefs(question)
+                )
+                if relevant_beliefs:
+                    logger.info(
+                        f"Injecting {len(relevant_beliefs)} relevant beliefs"
+                        " into prompt."
+                    )
+                    beliefs_prompt = self.template_manager.render(
+                        "framework.chat_manager.user_beliefs_prompt",
+                        {"beliefs": relevant_beliefs},
+                    )
+                    # Create a temporary, modified history for this turn
+                    turn_chat_history = self.chat_history.copy()
+                    original_system_prompt = turn_chat_history[0]["content"]
+                    # Append beliefs to the system prompt for this turn only
+                    turn_chat_history[0][
+                        "content"
+                    ] = f"{original_system_prompt}\n\n{beliefs_prompt}"
+
+            # --- LLM Call ---
             llm_response_stream = self.llm.chat(
-                chat_history=self.chat_history, stream=True
+                chat_history=turn_chat_history, stream=True
             )
 
             processed_answer = ""
@@ -973,7 +1010,7 @@ class ChatManager:
 
                 # Log assistant response to chat memory
                 if self.chat_memory:
-                    self.chat_memory.add_entry(
+                    assistant_message_id = self.chat_memory.add_entry(
                         processed_answer, "assistant"
                     )
             else:
@@ -983,7 +1020,7 @@ class ChatManager:
                     "No response generated", self.chat_history, "assistant"
                 )
                 if self.chat_memory:
-                    self.chat_memory.add_entry(
+                    assistant_message_id = self.chat_memory.add_entry(
                         "No response generated", "assistant"
                     )
 
@@ -998,6 +1035,9 @@ class ChatManager:
             if self.summary_enabled:
                 self._update_summary_in_background()
 
+            # We no longer update the profile on each turn
+            # Profile updates are now done in batch via process_new_messages_for_update()
+
             # For non-streaming mode, return the processed answer
             if not stream:
                 return processed_answer
diff --git a/ainara/framework/chat_memory.py b/ainara/framework/chat_memory.py
index 93acacd..61834c4 100644
--- a/ainara/framework/chat_memory.py
+++ b/ainara/framework/chat_memory.py
@@ -22,9 +22,9 @@ import os
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
+from ainara.framework.storage import get_text_backend, get_vector_backend
 # Import our storage backends
 from ainara.framework.storage.base import StorageBackend
-from ainara.framework.storage import get_text_backend, get_vector_backend
 
 logger = logging.getLogger(__name__)
 
@@ -67,7 +67,7 @@ class ChatMemory:
             text_type = config.get("memory.text_storage.type", "sqlite")
             text_path = config.get(
                 "memory.text_storage.storage_path",
-                os.path.join(config.get("data.directory"), "chat_memory.db")
+                os.path.join(config.get("data.directory"), "chat_memory.db"),
             )
 
             # Ensure path is expanded
@@ -76,12 +76,11 @@ class ChatMemory:
             # Create text backend
             try:
                 self.storage = get_text_backend(
-                    text_type,
-                    db_path=text_path,
-                    context_id=context_id
+                    text_type, db_path=text_path, context_id=context_id
                 )
                 logger.info(
-                    f"Using {text_type} storage backend with context {context_id}"
+                    f"Using {text_type} storage backend with context"
+                    f" {context_id}"
                 )
             except Exception as e:
                 logger.error(f"Failed to initialize {text_type} backend: {e}")
@@ -91,11 +90,11 @@ class ChatMemory:
         vector_type = config.get("memory.vector_storage.type", "chroma")
         vector_path = config.get(
             "memory.vector_db_path",
-            os.path.join(config.get("data.directory"), "vector_db")
+            os.path.join(config.get("data.directory"), "vector_db"),
         )
         embedding_model = config.get(
             "memory.vector_storage.embedding_model",
-            "sentence-transformers/all-mpnet-base-v2"
+            "sentence-transformers/all-mpnet-base-v2",
         )
 
         # Ensure path is expanded
@@ -105,13 +104,16 @@ class ChatMemory:
                 vector_type,
                 vector_db_path=vector_path,
                 embedding_model=embedding_model,
-                collection_name=context_id
+                collection_name=context_id,
             )
             logger.info(
                 f"Using {vector_type} vector backend with context {context_id}"
             )
         except ImportError:
-            logger.warning(f"Vector storage backend '{vector_type}' dependencies not found. Semantic search will be disabled.")
+            logger.warning(
+                f"Vector storage backend '{vector_type}' dependencies not"
+                " found. Semantic search will be disabled."
+            )
             self.vector_storage = None
         except Exception as e:
             logger.error(f"Failed to initialize vector storage: {e}")
@@ -121,6 +123,7 @@ class ChatMemory:
         self,
         content: str,
         role: str = "user",
+        source_type: str = "chat_history",
         user_id: Optional[str] = None,
         metadata: Optional[Dict[str, Any]] = None,
     ) -> str:
@@ -130,6 +133,7 @@ class ChatMemory:
         Args:
             content: The message content
             role: The role of the sender (user, assistant, system)
+            source_type: The origin of the content (e.g., 'chat_history', 'local_document')
             user_id: Optional user identifier (overrides context user)
             metadata: Additional metadata
 
@@ -139,9 +143,14 @@ class ChatMemory:
         # Create metadata with context information
         entry_metadata = metadata.copy() if metadata else {}
 
+        # Add the source type to the metadata
+        entry_metadata["source_type"] = source_type
+
         # Add a timestamp if one isn't already present. This is the authoritative timestamp.
         if "timestamp" not in entry_metadata:
-            entry_metadata["timestamp"] = datetime.now(timezone.utc).isoformat()
+            entry_metadata["timestamp"] = datetime.now(
+                timezone.utc
+            ).isoformat()
 
         # Add context information to metadata
         for key, value in self.context.items():
@@ -274,24 +283,24 @@ class ChatMemory:
             text_type = config.get("memory.text_storage.type", "sqlite")
             text_path = config.get(
                 "memory.text_storage.storage_path",
-                os.path.join(config.get("data.directory"), "chat_memory.db")
+                os.path.join(config.get("data.directory"), "chat_memory.db"),
             )
 
             # Create new text backend with new context
             self.storage = get_text_backend(
                 text_type,
                 db_path=os.path.expanduser(text_path),
-                context_id=new_context_id
+                context_id=new_context_id,
             )
 
             vector_type = config.get("memory.vector_storage.type", "chroma")
             vector_path = config.get(
                 "memory.vector_storage.storage_path",
-                os.path.join(config.get("data.directory"), "chat_memory.db")
+                os.path.join(config.get("data.directory"), "chat_memory.db"),
             )
             embedding_model = config.get(
                 "memory.vector_storage.embedding_model",
-                "sentence-transformers/all-mpnet-base-v2"
+                "sentence-transformers/all-mpnet-base-v2",
             )
 
             # Create new vector backend with new context
@@ -299,7 +308,7 @@ class ChatMemory:
                 vector_type,
                 vector_db_path=os.path.expanduser(vector_path),
                 embedding_model=embedding_model,
-                collection_name=new_context_id
+                collection_name=new_context_id,
             )
 
             # Update context
@@ -351,7 +360,9 @@ class ChatMemory:
         logger.info(f"Found {total_messages} messages to index.")
 
         for offset in range(0, total_messages, batch_size):
-            messages = self.storage.get_messages(limit=batch_size, offset=offset)
+            messages = self.storage.get_messages(
+                limit=batch_size, offset=offset
+            )
             if not messages:
                 break
 
@@ -372,7 +383,8 @@ class ChatMemory:
                 self.vector_storage.add_documents(documents_to_add)
 
             logger.info(
-                f"Indexed {offset + len(messages)} / {total_messages} messages."
+                f"Indexed {offset + len(messages)} /"
+                f" {total_messages} messages."
             )
 
         logger.info("Vector re-indexing complete.")
diff --git a/ainara/framework/storage/langchain_sqlite.py b/ainara/framework/storage/langchain_sqlite.py
index 2575b00..b672fae 100644
--- a/ainara/framework/storage/langchain_sqlite.py
+++ b/ainara/framework/storage/langchain_sqlite.py
@@ -227,6 +227,46 @@ class LangChainSQLiteStorage(StorageBackend):
             results.append(msg)
         return results
         
+    def get_message_by_id(self, message_id: str) -> Optional[Dict[str, Any]]:
+        """Get a single message by its ID."""
+        cursor = self.conn.cursor()
+        cursor.execute(
+            "SELECT * FROM messages WHERE id = ?", (message_id,)
+        )
+        row = cursor.fetchone()
+        
+        if not row:
+            return None
+            
+        msg = dict(row)
+        if msg.get("metadata"):
+            msg["metadata"] = json.loads(msg["metadata"])
+        return msg
+        
+    def get_messages_since(self, timestamp: Optional[str] = None) -> List[Dict[str, Any]]:
+        """Get all messages since a given timestamp."""
+        query = "SELECT * FROM messages WHERE context_id = ?"
+        params = [self.context_id]
+        
+        if timestamp:
+            query += " AND timestamp > ?"
+            params.append(timestamp)
+            
+        query += " ORDER BY timestamp ASC"
+        
+        cursor = self.conn.cursor()
+        cursor.execute(query, params)
+        rows = cursor.fetchall()
+        
+        # Convert rows to dictionaries and parse metadata
+        results = []
+        for row in rows:
+            msg = dict(row)
+            if msg.get("metadata"):
+                msg["metadata"] = json.loads(msg["metadata"])
+            results.append(msg)
+        return results
+
     def close(self):
         """Close any resources"""
         if self.conn:
diff --git a/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu b/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu
new file mode 100644
index 0000000..a138c08
--- /dev/null
+++ b/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu
@@ -0,0 +1,9 @@
+Here is a summary of what I know about the user, based on our past conversations. I will use this information to provide a more personalized and relevant response. Each belief includes a confidence score, a timestamp, and context tags to help me judge its importance and nature.
+
+**Known User Beliefs & Preferences:**
+{{#beliefs}}
+- **Belief:** {{belief}}
+  - **Confidence:** {{confidence}}
+  - **Last Mentioned:** {{last_updated}}
+  - **Context:** {{#context_tags}}{{.}}, {{/context_tags}}
+{{/beliefs}}
diff --git a/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu b/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu
new file mode 100644
index 0000000..de606a6
--- /dev/null
+++ b/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu
@@ -0,0 +1,22 @@
+You are a memory analysis system. Your task is to analyze a conversation snippet and determine if a meaningful, long-term belief, fact, or preference about the user can be extracted.
+
+- A "belief" is a core value, a goal, a strong opinion, or a significant personal fact.
+- Do NOT extract trivial or temporary information (e.g., "user is asking for the weather," "user wants to know the time").
+- If no meaningful belief can be extracted, you MUST respond with the single word: None
+
+If a belief can be extracted, you MUST respond with a single JSON object with the following structure:
+{
+  "topic": "A general category for the belief (e.g., 'work', 'hobbies', 'family', 'personal_goals', 'preferences').",
+  "belief": "The extracted belief, stated as a concise fact about the user.",
+  "confidence": A float between 0.0 and 1.0 indicating your confidence that this is a genuine, long-term belief.",
+  "context_tags": ["A list of strings describing the conversation's mood and nature (e.g., 'serious', 'joking', 'planning', 'work', 'personal', 'hypothetical', 'frustration')."]
+}
+
+---
+**Example 1: A clear, serious goal**
+
+**Conversation Snippet:**
+User: I've decided to dedicate the next six months to learning Rust programming. It's critical for my career advancement.
+Assistant: That's a great goal! Rust is a powerful language. I can help you find resources.
+
+**Your output:**
diff --git a/ainara/framework/user_profile_manager.py b/ainara/framework/user_profile_manager.py
new file mode 100644
index 0000000..c9927e4
--- /dev/null
+++ b/ainara/framework/user_profile_manager.py
@@ -0,0 +1,198 @@
+# Ainara AI Companion Framework Project
+# Copyright (C) 2025 Rubén Gómez - khromalabs.org
+#
+# This file is dual-licensed under:
+# 1. GNU Lesser General Public License v3.0 (LGPL-3.0)
+#    (See the included LICENSE_LGPL3.txt file or look into
+#    <https://www.gnu.org/licenses/lgpl-3.0.html> for details)
+# 2. Commercial license
+#    (Contact: rgomez@khromalabs.org for licensing options)
+#
+# You may use, distribute and modify this code under the terms of either license.
+# This notice must be preserved in all copies or substantial portions of the code.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+# Lesser General Public License for more details.
+
+
+import json
+import logging
+import os
+from datetime import datetime, timezone
+from typing import Any, Dict, List
+
+from ainara.framework.chat_memory import ChatMemory
+from ainara.framework.config import config
+from ainara.framework.llm.base import LLMBackend
+from ainara.framework.template_manager import TemplateManager
+
+logger = logging.getLogger(__name__)
+
+
+class UserProfileManager:
+    """Manages the user's semantic profile (beliefs, preferences, facts)."""
+
+    def __init__(self, llm: LLMBackend, chat_memory: ChatMemory):
+        self.llm = llm
+        self.chat_memory = chat_memory
+        self.profile_path = os.path.join(
+            config.get("data.directory"), "user_profile.json"
+        )
+        self.template_manager = TemplateManager()
+        self.profile = self._load_profile()
+
+    def _load_profile(self) -> Dict[str, Any]:
+        """Loads the user profile from disk, or creates a default one."""
+        if os.path.exists(self.profile_path):
+            try:
+                with open(self.profile_path, "r", encoding="utf-8") as f:
+                    logger.info(f"Loading user profile from {self.profile_path}")
+                    profile = json.load(f)
+                    # Ensure essential keys exist for backward compatibility
+                    profile.setdefault("key_beliefs", {})
+                    profile.setdefault("last_processed_timestamp", None)
+                    return profile
+            except (json.JSONDecodeError, IOError) as e:
+                logger.error(f"Error loading profile, creating a new one: {e}")
+        # Default structure for a new profile
+        return {"key_beliefs": {}, "last_processed_timestamp": None}
+
+    def _save_profile(self):
+        """Saves the current profile to disk."""
+        try:
+            with open(self.profile_path, "w", encoding="utf-8") as f:
+                json.dump(self.profile, f, indent=2, ensure_ascii=False)
+        except IOError as e:
+            logger.error(f"Failed to save user profile: {e}")
+
+    def get_relevant_beliefs(self, query: str, top_k: int = 3) -> List[Dict]:
+        """
+        Finds beliefs relevant to the user's query.
+        NOTE: This is a simple keyword-based search for the draft. A real
+        implementation should use semantic vector search on the beliefs for better accuracy.
+        """
+        relevant_beliefs = []
+        all_beliefs = [
+            b
+            for topic_beliefs in self.profile.get("key_beliefs", {}).values()
+            for b in topic_beliefs
+        ]
+
+        # Very simple relevance logic based on keyword matching
+        query_words = set(query.lower().split())
+        for belief in sorted(
+            all_beliefs, key=lambda b: b["last_updated"], reverse=True
+        ):
+            if any(word in belief["belief"].lower() for word in query_words):
+                if len(relevant_beliefs) < top_k:
+                    relevant_beliefs.append(belief)
+
+        return relevant_beliefs
+
+    def process_new_messages_for_update(self):
+        """
+        Fetches all new messages since the last update, processes them in
+        conversation turns, and updates the user profile.
+        """
+        last_timestamp = self.profile.get("last_processed_timestamp")
+        logger.info(
+            f"Starting profile update. Checking for messages since: {last_timestamp}"
+        )
+
+        # Fetch all messages since the last processed timestamp
+        new_messages = self.chat_memory.storage.get_messages_since(last_timestamp)
+
+        if not new_messages:
+            logger.info("No new messages to process for profile update.")
+            return
+
+        logger.info(f"Found {len(new_messages)} new messages to process.")
+
+        # Group messages into user/assistant turns
+        conversation_turns = []
+        for i, message in enumerate(new_messages):
+            if message.get("role") == "assistant" and i > 0:
+                prev_message = new_messages[i - 1]
+                if prev_message.get("role") == "user":
+                    conversation_turns.append((prev_message, message))
+
+        if not conversation_turns:
+            logger.info("No complete user/assistant turns found in new messages.")
+            # Update timestamp anyway to avoid reprocessing these single messages
+            self.profile["last_processed_timestamp"] = new_messages[-1].get(
+                "timestamp"
+            )
+            self._save_profile()
+            return
+
+        logger.info(f"Processing {len(conversation_turns)} new conversation turns.")
+        for user_msg, assistant_msg in conversation_turns:
+            self._extract_and_update_belief(user_msg, assistant_msg)
+
+        # After processing all turns, update the timestamp to the last message processed
+        latest_timestamp = new_messages[-1].get("timestamp")
+        self.profile["last_processed_timestamp"] = latest_timestamp
+        self._save_profile()
+        logger.info(
+            f"Profile update complete. New timestamp: {latest_timestamp}"
+        )
+
+    def _extract_and_update_belief(
+        self, user_message: Dict, assistant_message: Dict
+    ):
+        """
+        Runs the LLM on a single conversation turn to extract and save a belief.
+        """
+        try:
+            conversation_snippet = (
+                f"User: {user_message['content']}\n"
+                f"Assistant: {assistant_message['content']}"
+            )
+
+            prompt = self.template_manager.render(
+                "framework.user_profile_manager.extract_belief",
+                {"conversation_snippet": conversation_snippet},
+            )
+
+            # Use a dedicated history for this one-off task
+            extraction_history = []
+            self.llm.add_msg(
+                "You are a helpful memory analysis system that extracts structured data from conversations.",
+                extraction_history,
+                "system",
+            )
+            self.llm.add_msg(prompt, extraction_history, "user")
+
+            response_str = self.llm.chat(chat_history=extraction_history, stream=False)
+
+            # The LLM should return a JSON object or "None"
+            if response_str.strip().lower() == "none":
+                logger.info("LLM determined no new belief to be extracted.")
+                return
+
+            new_belief_data = json.loads(response_str)
+            topic = new_belief_data.get("topic", "general")
+
+            # Add provenance and timestamp
+            new_belief_data["source_message_ids"] = [
+                user_message.get("id"),
+                assistant_message.get("id"),
+            ]
+            new_belief_data["last_updated"] = datetime.now(timezone.utc).isoformat()
+
+            # Add to profile (non-destructive append-only model)
+            if topic not in self.profile["key_beliefs"]:
+                self.profile["key_beliefs"][topic] = []
+            self.profile["key_beliefs"][topic].append(new_belief_data)
+
+            self._save_profile()
+            logger.info(f"Updated user profile with new belief on topic: {topic}")
+
+        except json.JSONDecodeError:
+            logger.warning(f"LLM returned invalid JSON for belief extraction: {response_str}")
+        except Exception as e:
+            logger.error(f"Failed to update user profile from conversation: {e}")
+
+
diff --git a/diff b/diff
new file mode 100644
index 0000000..55ec2d8
--- /dev/null
+++ b/diff
@@ -0,0 +1,477 @@
+diff --git a/ainara/framework/chat_manager.py b/ainara/framework/chat_manager.py
+index bb47c4c..dce95ee 100644
+--- a/ainara/framework/chat_manager.py
++++ b/ainara/framework/chat_manager.py
+@@ -38,6 +38,7 @@ from ainara.framework.loading_animation import LoadingAnimation
+ from ainara.framework.orakle_middleware import OrakleMiddleware
+ from ainara.framework.template_manager import TemplateManager
+ from ainara.framework.tts.base import TTSBackend
++from ainara.framework.user_profile_manager import UserProfileManager
+ from ainara.framework.utils import load_spacy_model
+ 
+ # import pprint
+@@ -105,6 +106,15 @@ class ChatManager:
+             self.chat_memory = None
+             logger.info("Chat memory disabled")
+ 
++        # Initialize User Profile Manager
++        if self.chat_memory:
++            self.user_profile_manager = UserProfileManager(
++                llm=self.llm, chat_memory=self.chat_memory
++            )
++            logger.info("User Profile Manager initialized")
++        else:
++            self.user_profile_manager = None
++
+         # Render the system message template
+         current_date = datetime.now().date()
+         self.system_message = self.template_manager.render(
+@@ -768,6 +778,8 @@ class ChatManager:
+     def chat_completion(
+         self, question: str, stream: Optional[Literal["cli", "json"]] = "cli"
+     ) -> Union[str, Generator[str, None, None], dict]:
++        user_message_id = None
++        assistant_message_id = None
+         """Main chat completion function
+ 
+         Args:
+@@ -810,7 +822,7 @@ class ChatManager:
+         processed_answer = ""
+         try:
+             if self.chat_memory:
+-                self.chat_memory.add_entry(question, "user")
++                user_message_id = self.chat_memory.add_entry(question, "user")
+ 
+             # Check if the last message is from a user, and if so, log a warning
+             if (
+@@ -843,8 +855,33 @@ class ChatManager:
+                 self.chat_history[1]["tokens"] = new_summary_tokens
+                 logger.info(f"Applied new summary: {new_summary_copy[:50]}...")
+             self.trim_context()
++
++            # --- User Profile Injection ---
++            turn_chat_history = self.chat_history
++            if self.user_profile_manager:
++                relevant_beliefs = (
++                    self.user_profile_manager.get_relevant_beliefs(question)
++                )
++                if relevant_beliefs:
++                    logger.info(
++                        f"Injecting {len(relevant_beliefs)} relevant beliefs"
++                        " into prompt."
++                    )
++                    beliefs_prompt = self.template_manager.render(
++                        "framework.chat_manager.user_beliefs_prompt",
++                        {"beliefs": relevant_beliefs},
++                    )
++                    # Create a temporary, modified history for this turn
++                    turn_chat_history = self.chat_history.copy()
++                    original_system_prompt = turn_chat_history[0]["content"]
++                    # Append beliefs to the system prompt for this turn only
++                    turn_chat_history[0][
++                        "content"
++                    ] = f"{original_system_prompt}\n\n{beliefs_prompt}"
++
++            # --- LLM Call ---
+             llm_response_stream = self.llm.chat(
+-                chat_history=self.chat_history, stream=True
++                chat_history=turn_chat_history, stream=True
+             )
+ 
+             processed_answer = ""
+@@ -973,7 +1010,7 @@ class ChatManager:
+ 
+                 # Log assistant response to chat memory
+                 if self.chat_memory:
+-                    self.chat_memory.add_entry(
++                    assistant_message_id = self.chat_memory.add_entry(
+                         processed_answer, "assistant"
+                     )
+             else:
+@@ -983,7 +1020,7 @@ class ChatManager:
+                     "No response generated", self.chat_history, "assistant"
+                 )
+                 if self.chat_memory:
+-                    self.chat_memory.add_entry(
++                    assistant_message_id = self.chat_memory.add_entry(
+                         "No response generated", "assistant"
+                     )
+ 
+@@ -998,6 +1035,9 @@ class ChatManager:
+             if self.summary_enabled:
+                 self._update_summary_in_background()
+ 
++            # We no longer update the profile on each turn
++            # Profile updates are now done in batch via process_new_messages_for_update()
++
+             # For non-streaming mode, return the processed answer
+             if not stream:
+                 return processed_answer
+diff --git a/ainara/framework/chat_memory.py b/ainara/framework/chat_memory.py
+index 93acacd..61834c4 100644
+--- a/ainara/framework/chat_memory.py
++++ b/ainara/framework/chat_memory.py
+@@ -22,9 +22,9 @@ import os
+ from datetime import datetime, timezone
+ from typing import Any, Dict, List, Optional
+ 
++from ainara.framework.storage import get_text_backend, get_vector_backend
+ # Import our storage backends
+ from ainara.framework.storage.base import StorageBackend
+-from ainara.framework.storage import get_text_backend, get_vector_backend
+ 
+ logger = logging.getLogger(__name__)
+ 
+@@ -67,7 +67,7 @@ class ChatMemory:
+             text_type = config.get("memory.text_storage.type", "sqlite")
+             text_path = config.get(
+                 "memory.text_storage.storage_path",
+-                os.path.join(config.get("data.directory"), "chat_memory.db")
++                os.path.join(config.get("data.directory"), "chat_memory.db"),
+             )
+ 
+             # Ensure path is expanded
+@@ -76,12 +76,11 @@ class ChatMemory:
+             # Create text backend
+             try:
+                 self.storage = get_text_backend(
+-                    text_type,
+-                    db_path=text_path,
+-                    context_id=context_id
++                    text_type, db_path=text_path, context_id=context_id
+                 )
+                 logger.info(
+-                    f"Using {text_type} storage backend with context {context_id}"
++                    f"Using {text_type} storage backend with context"
++                    f" {context_id}"
+                 )
+             except Exception as e:
+                 logger.error(f"Failed to initialize {text_type} backend: {e}")
+@@ -91,11 +90,11 @@ class ChatMemory:
+         vector_type = config.get("memory.vector_storage.type", "chroma")
+         vector_path = config.get(
+             "memory.vector_db_path",
+-            os.path.join(config.get("data.directory"), "vector_db")
++            os.path.join(config.get("data.directory"), "vector_db"),
+         )
+         embedding_model = config.get(
+             "memory.vector_storage.embedding_model",
+-            "sentence-transformers/all-mpnet-base-v2"
++            "sentence-transformers/all-mpnet-base-v2",
+         )
+ 
+         # Ensure path is expanded
+@@ -105,13 +104,16 @@ class ChatMemory:
+                 vector_type,
+                 vector_db_path=vector_path,
+                 embedding_model=embedding_model,
+-                collection_name=context_id
++                collection_name=context_id,
+             )
+             logger.info(
+                 f"Using {vector_type} vector backend with context {context_id}"
+             )
+         except ImportError:
+-            logger.warning(f"Vector storage backend '{vector_type}' dependencies not found. Semantic search will be disabled.")
++            logger.warning(
++                f"Vector storage backend '{vector_type}' dependencies not"
++                " found. Semantic search will be disabled."
++            )
+             self.vector_storage = None
+         except Exception as e:
+             logger.error(f"Failed to initialize vector storage: {e}")
+@@ -121,6 +123,7 @@ class ChatMemory:
+         self,
+         content: str,
+         role: str = "user",
++        source_type: str = "chat_history",
+         user_id: Optional[str] = None,
+         metadata: Optional[Dict[str, Any]] = None,
+     ) -> str:
+@@ -130,6 +133,7 @@ class ChatMemory:
+         Args:
+             content: The message content
+             role: The role of the sender (user, assistant, system)
++            source_type: The origin of the content (e.g., 'chat_history', 'local_document')
+             user_id: Optional user identifier (overrides context user)
+             metadata: Additional metadata
+ 
+@@ -139,9 +143,14 @@ class ChatMemory:
+         # Create metadata with context information
+         entry_metadata = metadata.copy() if metadata else {}
+ 
++        # Add the source type to the metadata
++        entry_metadata["source_type"] = source_type
++
+         # Add a timestamp if one isn't already present. This is the authoritative timestamp.
+         if "timestamp" not in entry_metadata:
+-            entry_metadata["timestamp"] = datetime.now(timezone.utc).isoformat()
++            entry_metadata["timestamp"] = datetime.now(
++                timezone.utc
++            ).isoformat()
+ 
+         # Add context information to metadata
+         for key, value in self.context.items():
+@@ -274,24 +283,24 @@ class ChatMemory:
+             text_type = config.get("memory.text_storage.type", "sqlite")
+             text_path = config.get(
+                 "memory.text_storage.storage_path",
+-                os.path.join(config.get("data.directory"), "chat_memory.db")
++                os.path.join(config.get("data.directory"), "chat_memory.db"),
+             )
+ 
+             # Create new text backend with new context
+             self.storage = get_text_backend(
+                 text_type,
+                 db_path=os.path.expanduser(text_path),
+-                context_id=new_context_id
++                context_id=new_context_id,
+             )
+ 
+             vector_type = config.get("memory.vector_storage.type", "chroma")
+             vector_path = config.get(
+                 "memory.vector_storage.storage_path",
+-                os.path.join(config.get("data.directory"), "chat_memory.db")
++                os.path.join(config.get("data.directory"), "chat_memory.db"),
+             )
+             embedding_model = config.get(
+                 "memory.vector_storage.embedding_model",
+-                "sentence-transformers/all-mpnet-base-v2"
++                "sentence-transformers/all-mpnet-base-v2",
+             )
+ 
+             # Create new vector backend with new context
+@@ -299,7 +308,7 @@ class ChatMemory:
+                 vector_type,
+                 vector_db_path=os.path.expanduser(vector_path),
+                 embedding_model=embedding_model,
+-                collection_name=new_context_id
++                collection_name=new_context_id,
+             )
+ 
+             # Update context
+@@ -351,7 +360,9 @@ class ChatMemory:
+         logger.info(f"Found {total_messages} messages to index.")
+ 
+         for offset in range(0, total_messages, batch_size):
+-            messages = self.storage.get_messages(limit=batch_size, offset=offset)
++            messages = self.storage.get_messages(
++                limit=batch_size, offset=offset
++            )
+             if not messages:
+                 break
+ 
+@@ -372,7 +383,8 @@ class ChatMemory:
+                 self.vector_storage.add_documents(documents_to_add)
+ 
+             logger.info(
+-                f"Indexed {offset + len(messages)} / {total_messages} messages."
++                f"Indexed {offset + len(messages)} /"
++                f" {total_messages} messages."
+             )
+ 
+         logger.info("Vector re-indexing complete.")
+diff --git a/ainara/framework/storage/langchain_sqlite.py b/ainara/framework/storage/langchain_sqlite.py
+index 2575b00..b672fae 100644
+--- a/ainara/framework/storage/langchain_sqlite.py
++++ b/ainara/framework/storage/langchain_sqlite.py
+@@ -227,6 +227,46 @@ class LangChainSQLiteStorage(StorageBackend):
+             results.append(msg)
+         return results
+         
++    def get_message_by_id(self, message_id: str) -> Optional[Dict[str, Any]]:
++        """Get a single message by its ID."""
++        cursor = self.conn.cursor()
++        cursor.execute(
++            "SELECT * FROM messages WHERE id = ?", (message_id,)
++        )
++        row = cursor.fetchone()
++        
++        if not row:
++            return None
++            
++        msg = dict(row)
++        if msg.get("metadata"):
++            msg["metadata"] = json.loads(msg["metadata"])
++        return msg
++        
++    def get_messages_since(self, timestamp: Optional[str] = None) -> List[Dict[str, Any]]:
++        """Get all messages since a given timestamp."""
++        query = "SELECT * FROM messages WHERE context_id = ?"
++        params = [self.context_id]
++        
++        if timestamp:
++            query += " AND timestamp > ?"
++            params.append(timestamp)
++            
++        query += " ORDER BY timestamp ASC"
++        
++        cursor = self.conn.cursor()
++        cursor.execute(query, params)
++        rows = cursor.fetchall()
++        
++        # Convert rows to dictionaries and parse metadata
++        results = []
++        for row in rows:
++            msg = dict(row)
++            if msg.get("metadata"):
++                msg["metadata"] = json.loads(msg["metadata"])
++            results.append(msg)
++        return results
++
+     def close(self):
+         """Close any resources"""
+         if self.conn:
+diff --git a/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu b/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu
+new file mode 100644
+index 0000000..a138c08
+--- /dev/null
++++ b/ainara/framework/templates/framework/chat_manager/user_beliefs_prompt.mu
+@@ -0,0 +1,9 @@
++Here is a summary of what I know about the user, based on our past conversations. I will use this information to provide a more personalized and relevant response. Each belief includes a confidence score, a timestamp, and context tags to help me judge its importance and nature.
++
++**Known User Beliefs & Preferences:**
++{{#beliefs}}
++- **Belief:** {{belief}}
++  - **Confidence:** {{confidence}}
++  - **Last Mentioned:** {{last_updated}}
++  - **Context:** {{#context_tags}}{{.}}, {{/context_tags}}
++{{/beliefs}}
+diff --git a/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu b/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu
+new file mode 100644
+index 0000000..de606a6
+--- /dev/null
++++ b/ainara/framework/templates/framework/user_profile_manager/extract_belief.mu
+@@ -0,0 +1,22 @@
++You are a memory analysis system. Your task is to analyze a conversation snippet and determine if a meaningful, long-term belief, fact, or preference about the user can be extracted.
++
++- A "belief" is a core value, a goal, a strong opinion, or a significant personal fact.
++- Do NOT extract trivial or temporary information (e.g., "user is asking for the weather," "user wants to know the time").
++- If no meaningful belief can be extracted, you MUST respond with the single word: None
++
++If a belief can be extracted, you MUST respond with a single JSON object with the following structure:
++{
++  "topic": "A general category for the belief (e.g., 'work', 'hobbies', 'family', 'personal_goals', 'preferences').",
++  "belief": "The extracted belief, stated as a concise fact about the user.",
++  "confidence": A float between 0.0 and 1.0 indicating your confidence that this is a genuine, long-term belief.",
++  "context_tags": ["A list of strings describing the conversation's mood and nature (e.g., 'serious', 'joking', 'planning', 'work', 'personal', 'hypothetical', 'frustration')."]
++}
++
++---
++**Example 1: A clear, serious goal**
++
++**Conversation Snippet:**
++User: I've decided to dedicate the next six months to learning Rust programming. It's critical for my career advancement.
++Assistant: That's a great goal! Rust is a powerful language. I can help you find resources.
++
++**Your output:**
+diff --git a/ainara/framework/user_profile_manager.py b/ainara/framework/user_profile_manager.py
+new file mode 100644
+index 0000000..c9927e4
+--- /dev/null
++++ b/ainara/framework/user_profile_manager.py
+@@ -0,0 +1,198 @@
++# Ainara AI Companion Framework Project
++# Copyright (C) 2025 Rubén Gómez - khromalabs.org
++#
++# This file is dual-licensed under:
++# 1. GNU Lesser General Public License v3.0 (LGPL-3.0)
++#    (See the included LICENSE_LGPL3.txt file or look into
++#    <https://www.gnu.org/licenses/lgpl-3.0.html> for details)
++# 2. Commercial license
++#    (Contact: rgomez@khromalabs.org for licensing options)
++#
++# You may use, distribute and modify this code under the terms of either license.
++# This notice must be preserved in all copies or substantial portions of the code.
++#
++# This program is distributed in the hope that it will be useful,
++# but WITHOUT ANY WARRANTY; without even the implied warranty of
++# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
++# Lesser General Public License for more details.
++
++
++import json
++import logging
++import os
++from datetime import datetime, timezone
++from typing import Any, Dict, List
++
++from ainara.framework.chat_memory import ChatMemory
++from ainara.framework.config import config
++from ainara.framework.llm.base import LLMBackend
++from ainara.framework.template_manager import TemplateManager
++
++logger = logging.getLogger(__name__)
++
++
++class UserProfileManager:
++    """Manages the user's semantic profile (beliefs, preferences, facts)."""
++
++    def __init__(self, llm: LLMBackend, chat_memory: ChatMemory):
++        self.llm = llm
++        self.chat_memory = chat_memory
++        self.profile_path = os.path.join(
++            config.get("data.directory"), "user_profile.json"
++        )
++        self.template_manager = TemplateManager()
++        self.profile = self._load_profile()
++
++    def _load_profile(self) -> Dict[str, Any]:
++        """Loads the user profile from disk, or creates a default one."""
++        if os.path.exists(self.profile_path):
++            try:
++                with open(self.profile_path, "r", encoding="utf-8") as f:
++                    logger.info(f"Loading user profile from {self.profile_path}")
++                    profile = json.load(f)
++                    # Ensure essential keys exist for backward compatibility
++                    profile.setdefault("key_beliefs", {})
++                    profile.setdefault("last_processed_timestamp", None)
++                    return profile
++            except (json.JSONDecodeError, IOError) as e:
++                logger.error(f"Error loading profile, creating a new one: {e}")
++        # Default structure for a new profile
++        return {"key_beliefs": {}, "last_processed_timestamp": None}
++
++    def _save_profile(self):
++        """Saves the current profile to disk."""
++        try:
++            with open(self.profile_path, "w", encoding="utf-8") as f:
++                json.dump(self.profile, f, indent=2, ensure_ascii=False)
++        except IOError as e:
++            logger.error(f"Failed to save user profile: {e}")
++
++    def get_relevant_beliefs(self, query: str, top_k: int = 3) -> List[Dict]:
++        """
++        Finds beliefs relevant to the user's query.
++        NOTE: This is a simple keyword-based search for the draft. A real
++        implementation should use semantic vector search on the beliefs for better accuracy.
++        """
++        relevant_beliefs = []
++        all_beliefs = [
++            b
++            for topic_beliefs in self.profile.get("key_beliefs", {}).values()
++            for b in topic_beliefs
++        ]
++
++        # Very simple relevance logic based on keyword matching
++        query_words = set(query.lower().split())
++        for belief in sorted(
++            all_beliefs, key=lambda b: b["last_updated"], reverse=True
++        ):
++            if any(word in belief["belief"].lower() for word in query_words):
++                if len(relevant_beliefs) < top_k:
++                    relevant_beliefs.append(belief)
++
++        return relevant_beliefs
++
++    def process_new_messages_for_update(self):
++        """
++        Fetches all new messages since the last update, processes them in
++        conversation turns, and updates the user profile.
++        """
++        last_timestamp = self.profile.get("last_processed_timestamp")
++        logger.info(
++            f"Starting profile update. Checking for messages since: {last_timestamp}"
++        )
++
++        # Fe
\ No newline at end of file
